{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "# from datetime import datetime\n",
    "from scipy import stats\n",
    "# from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料儲存路徑&參數設置\n",
    "initial_place = \"D:/user/文件/python/Eric/\"\n",
    "# 測試時需要確認此日期在資料夾內 \n",
    "Start_date = \"20110614\"\n",
    "start_file = 'FTPOSF_'+Start_date+'.csv'\n",
    "End_date = \"20110614\"\n",
    "end_file = 'FTPOSF_'+End_date+'.csv'\n",
    "\n",
    "poi_file_start = 'FTPPOI_'+ Start_date[:6]+'.t.csv'\n",
    "poi_file_end = 'FTPPOI_'+ End_date[:6]+'.t.csv'\n",
    "\n",
    "name = [\"OSF\",\"MTF\",\"DSP\",\"POI\"]\n",
    "poi_count = 0\n",
    "poi_true = True\n",
    "year_month_now = \"\"\n",
    "date = \"\"\n",
    "date_now = \"\"\n",
    "Folder_Path_ini = initial_place + \"\\\\\" + \"OSF\"\n",
    "os.chdir(Folder_Path_ini)\n",
    "file_list = np.array(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#挑出TXF和單式委託\n",
    "def Data_Pick():\n",
    "    OSF_TXF = OSF_RawData[(OSF_RawData['merchandise'] == 'TXF')&(OSF_RawData['sc']=='S')].reset_index(drop=True)\n",
    "    MTF_TXF = MTF_RawData[(MTF_RawData['merchandise'] == 'TXF')&(MTF_RawData['sc']=='S')].reset_index(drop=True)\n",
    "    DSP_TXF = DSP_RawData[(DSP_RawData['merchandise'] == 'TXF')&(DSP_RawData['sc']=='S')].reset_index(drop=True)\n",
    "    POI_TXF = POI_RawData[(POI_RawData['merchandise'] == 'TXF')].reset_index(drop=True)     \n",
    "\n",
    "    OSF_TXF = OSF_TXF.sort_values(by=['daytime']).reset_index(drop=True)\n",
    "    #columns 預處理\n",
    "    OSF_TXF['date'] = OSF_TXF['daytime'].dt.date\n",
    "    OSF_TXF['time'] = OSF_TXF['daytime'].dt.time\n",
    "\n",
    "    MTF_TXF['date'] = MTF_TXF['daytime'].dt.date\n",
    "\n",
    "    POI_TXF['TotalVol'] = POI_TXF['tbuyv'] + POI_TXF['tsellv']\n",
    "    POI_TXF['TotalPOI'] = POI_TXF['tbuyre'] + POI_TXF['tsellre']\n",
    "    POI_TXF['date'] = POI_TXF['daytime'].dt.date\n",
    "\n",
    "    DSP_TXF['date'] = DSP_TXF['daytime'].dt.date\n",
    "    return OSF_TXF, MTF_TXF, DSP_TXF, POI_TXF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#每月判斷一次轉倉日期避掉迴圈\n",
    "def Transfer():\n",
    "    Transdaylist1 = [20070118, 20070227, 20070322, 20070419, 20070517, 20070621, 20070719, 20070816, 20070920, 20071018,\n",
    "                   20071122, 20071220, 20080117, 20080221, 20080320, 20080417, 20080522, 20080619, 20080717, 20080821,\n",
    "                   20080918, 20081016, 20081120, 20081217, 20090121, 20090218, 20090318, 20090415, 20090520, 20090617,\n",
    "                   20090715, 20090819, 20090916, 20091021, 20091118, 20091216, 20100120, 20100222, 20100317, 20100421,\n",
    "                   20100519, 20100617, 20100721, 20100818, 20100915, 20101020, 20101117, 20101215, 20110119, 20110216,\n",
    "                   20110316, 20110420, 20110518, 20110615, 20110720, 20110817, 20110921, 20111019, 20111116, 20111221]\n",
    "    labels = ['200702', '200703', '200704', '200705', '200706', '200707', '200708', '200709', '200710',\n",
    "              '200711', '200712', '200801', '200802', '200803', '200804', '200805', '200806', '200807', '200808',\n",
    "              '200809', '200810', '200811', '200812', '200901', '200902', '200903', '200904', '200905', '200906',\n",
    "              '200907', '200908', '200909', '200910', '200911', '200912', '201001', '201002', '201003', '201004',\n",
    "              '201005', '201006', '201007', '201008', '201009', '201010', '201011', '201012', '201101', '201102',\n",
    "              '201103', '201104', '201105', '201106', '201107', '201108', '201109', '201110', '201111', '201112']\n",
    "    OSF_TXF1 = OSF_TXF\n",
    "    OSF_TXF1['date_cut'] = OSF_TXF1['date'].apply(lambda x : int(x.strftime('%Y%m%d')))\n",
    "    OSF_TXF1['labels'] = pd.cut(OSF_TXF1['date_cut'], Transdaylist1, right = False, labels = labels)\n",
    "    OSF_TXF1['contract1'] = OSF_TXF1['contract1'].apply(lambda x: str(x))\n",
    "    OSF_target = OSF_TXF1[OSF_TXF1['contract1']==OSF_TXF1['labels']]\n",
    "    OSF_target = OSF_target.drop(['labels','date_cut'],axis= 1).reset_index(drop=True)\n",
    "\n",
    "    POI_TXF1 = POI_TXF\n",
    "    POI_TXF1['date_cut'] = POI_TXF1['date'].apply(lambda x : int(x.strftime('%Y%m%d')))\n",
    "    POI_TXF1['labels'] = pd.cut(POI_TXF1['date_cut'], Transdaylist1, right = False, labels = labels)\n",
    "    POI_TXF1['contract'] = POI_TXF1['contract'].apply(lambda x: str(x))\n",
    "    POI_target = POI_TXF1[(POI_TXF1['contract']==POI_TXF1['labels'])&(POI_TXF1['TotalVol']>0)]\n",
    "    POI_target = POI_target.drop(['labels','date_cut'],axis= 1).reset_index(drop=True)\n",
    "\n",
    "    MTF_TXF1 = MTF_TXF\n",
    "    MTF_TXF1['date_cut'] = MTF_TXF1['date'].apply(lambda x : int(x.strftime('%Y%m%d')))\n",
    "    MTF_TXF1['labels'] = pd.cut(MTF_TXF1['date_cut'], Transdaylist1, right = False, labels = labels)\n",
    "    MTF_TXF1['contract1'] = MTF_TXF1['contract1'].apply(lambda x: str(x))\n",
    "    MTF_target = MTF_TXF1[MTF_TXF1['contract1']==MTF_TXF1['labels']]\n",
    "    MTF_target = MTF_target.drop(['labels','date_cut'],axis= 1).reset_index(drop=True)\n",
    "\n",
    "    DSP_TXF1 = DSP_TXF\n",
    "    DSP_TXF1['date_cut'] = DSP_TXF1['date'].apply(lambda x : int(x.strftime('%Y%m%d')))\n",
    "    DSP_TXF1['labels'] = pd.cut(DSP_TXF1['date_cut'], Transdaylist1, right = False, labels = labels)\n",
    "    DSP_TXF1['contract1'] = DSP_TXF1['contract1'].apply(lambda x: str(x))\n",
    "    DSP_target = DSP_TXF1[DSP_TXF1['contract1']==DSP_TXF1['labels']]\n",
    "    DSP_target = DSP_target.drop(['labels','date_cut'],axis= 1).reset_index(drop=True)\n",
    "    return OSF_target, POI_target, MTF_target, DSP_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#運算並記錄成交量/委託量/執行率/持倉量/持倉時間\n",
    "def Vol_Calculate():\n",
    "    #運算並記錄成交量/委託量/執行率/持倉量/持倉時間\n",
    "    EexeVol = MTF_target.groupby(['date','id'])['vol1'].sum()       \n",
    "    EexeVol = EexeVol.reset_index()                                 #成交從MTF檔中加總計算\n",
    "\n",
    "    OrderVol = OSF_target.groupby(['date','id'])['vol'].sum() \n",
    "    OrderVol = OrderVol.reset_index()                               #委託數量從OSF檔中計算有效委託數量\n",
    "\n",
    "    Record_df = pd.merge(EexeVol,OrderVol, on=['date','id'], how='outer').rename(columns={\"vol1\": \"ExeVol\", 'vol': \"OrderVol\"}).fillna(0)\n",
    "    Record_df['ExeRate'] = Record_df['ExeVol']/Record_df['OrderVol']  #將成交及有效委託數量merge之後再計算委託/成交比\n",
    "\n",
    "    #將存貨檔與委託檔合併挑出日內有委託or有成交or有部位的交易人\n",
    "    #這個資料即會將持有部位但是未委託和成交的交易人也納入須要進一步討論\n",
    "#     HoldingPeriod_temp = HoldingPeriod[(HoldingPeriod['date']==datetime.date(2011,6,14))|(HoldingPeriod['date']==datetime.date(2011,6,15))|(HoldingPeriod['date']==datetime.date(2011,6,16))]   #平均持倉時間是比較健康的篩選條件\n",
    "#     HoldingPeriod_temp = HoldingPeriod_temp[['date','id','HoldingPeriod']]\n",
    "\n",
    "    Record_df = pd.merge(Record_df,HoldingPeriod, on=['date','id'], how = 'outer')  \n",
    "    Record_df = Record_df.fillna(0)\n",
    "    Record_df = Record_df[Record_df['HoldingPeriod'] > datetime.timedelta(0)]              #剔除有委託沒成交沒持倉的交易人\n",
    "\n",
    "    #(高委託/成交比+高交易量+低庫存)挑出每日快速交易者\n",
    "    Record_df1 = Record_df\n",
    "    Datelist1 = Record_df1['date'].unique()\n",
    "    RankData = pd.DataFrame(columns = Record_df1.columns)        #建一個儲存器\n",
    "    for i in Datelist1:\n",
    "        Datatoday = Record_df1[Record_df1['date']==i]\n",
    "        Datatoday['ExeVolScore'] = Datatoday['ExeVol'].apply(lambda x: stats.percentileofscore(Datatoday['ExeVol'], x))\n",
    "        Datatoday['OrderVolScore'] = Datatoday['OrderVol'].apply(lambda x: stats.percentileofscore(Datatoday['OrderVol'], x))\n",
    "        Datatoday['ExeRateScore'] = Datatoday['ExeRate'].apply(lambda x: stats.percentileofscore(Datatoday['ExeRate'], x))\n",
    "        Datatoday['HoldingPeriod'] = Datatoday['HoldingPeriod'].apply(lambda x:x.total_seconds())\n",
    "        Datatoday['HoldingPeriodScore'] = Datatoday['HoldingPeriod'].apply(lambda x: stats.percentileofscore(Datatoday['HoldingPeriod'], x)) \n",
    "        RankData = RankData.append(Datatoday)\n",
    "        Datatoday = []\n",
    "\n",
    "    #記錄每日快速交易者\n",
    "    Datelist = pd.unique(POI_target['date'])\n",
    "    Threshold = 20\n",
    "    RankData_HFT = RankData[(RankData['HoldingPeriodScore']<=Threshold)&(RankData['OrderVolScore']>=(100-Threshold))&\n",
    "        (RankData['ExeVolScore']>=(100-Threshold))&(RankData['ExeRateScore']<=Threshold)]\n",
    "    Record_HFT = RankData_HFT[['date','id']].reset_index(drop=True)\n",
    "    return Record_HFT,RankData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#篩選出每日快速交易人\n",
    "# Threshold = [0,10,20,30,40,50,60,70,80,90]\n",
    "def Features_Record(name):\n",
    "    #紀錄交易人特性基本統計量\n",
    "    Threshold = [10,20,30,40,50,60,70,80,90]\n",
    "    Features_record = pd.DataFrame(columns =['HoldingPeriod','OrderVol', 'ExeVol', 'ExeRate'],index=range(len(Threshold)))\n",
    "    for i in range(len(Threshold)):\n",
    "        condition1 = (RankData['HoldingPeriodScore']<= Threshold[i])\n",
    "        condition2 = (RankData['OrderVolScore']>= Threshold[8-i])\n",
    "        condition3 = (RankData['ExeVolScore']>= Threshold[8-i])\n",
    "        condition4 = (RankData['ExeRateScore']<= Threshold[i])\n",
    "        Features_record.loc[i,'HoldingPeriod'] = RankData[(condition1)]['HoldingPeriod'].mean()\n",
    "        Features_record.loc[i,'OrderVol'] = RankData[(condition2)]['OrderVol'].mean()\n",
    "        Features_record.loc[i,'ExeVol'] = RankData[(condition3)]['OrderVol'].mean()\n",
    "        Features_record.loc[i,'ExeRate'] = RankData[(condition4)]['ExeRate'].mean()\n",
    "        try:\n",
    "            Features_record.loc[i,'Order/ExeRatio']= 1/Features_record.loc[i,'ExeRate']\n",
    "        except:\n",
    "            Features_record.loc[i,'Order/ExeRatio']=0\n",
    "    Features_record.to_csv('Features_record'+ date +name+'.csv')\n",
    "    return Features_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#切割MTF檔計算HFT參與率\n",
    "# 切割所屬樣本\n",
    "#先創一段時間樣本讓並記錄OSF委託所屬樣本區間\n",
    "def JoinRate5M():\n",
    "    Time_list = [845, 850, 855, 900, 905 ,910, 915, 920, 925, 930, 935, 940, 945, 950, 955, 1000, 1005, 1010, 1015, 1020, 1025, 1030, 1035,\n",
    "                1040, 1045, 1050, 1055, 1100, 1105, 1110, 1115, 1120, 1125, 1130, 1135, 1140, 1145, 1150, 1155, 1200, 1205, 1210,1215,\n",
    "                1220, 1225, 1230, 1235, 1240, 1245, 1250, 1255, 1300, 1305, 1310, 1315, 1320, 1325, 1330, 1335, 1340, 1346]\n",
    "    Time_labels =['850', '855', \"900\", \"905\" ,\"910\", \"915\", \"920\", \"925\", \"930\", \"935\", \"940\", \"945\", \"950\", \"955\", \"1000\", \"1005\", \"1010\",\n",
    "                  \"1015\", \"1020\", \"1025\", \"1030\", \"1035\", \"1040\", \"1045\", \"1050\", \"1055\", \"1100\", \"1105\", \"1110\", '1115', '1120', '1125', \n",
    "                  '1130', '1135', '1140', '1145', '1150', '1155', '1200', '1205', '1210', '1215', '1220', '1225', '1230', '1235', '1240',\n",
    "                  '1245', '1250', '1255', '1300', '1305', '1310', '1315', '1320', '1325', '1330', '1335', '1340', '1346']\n",
    "    MTF_target2 = MTF_target.copy()\n",
    "    MTF_target2['time'] = MTF_target2['daytime'].apply(lambda x: (x.hour)*100+x.minute)\n",
    "    MTF_target2['Time_labels'] = pd.cut(MTF_target2['time'], Time_list, right = False, labels = Time_labels)\n",
    "\n",
    "    #貼標記給所有HFT後計算參與率\n",
    "    MTF_target2['HFT'] = 0\n",
    "    for i in range(len(Record_HFT)):\n",
    "        MTF_target2.loc[(Record_HFT.loc[i,'date']==MTF_target2['date'])&(Record_HFT.loc[i,'id']==MTF_target2['id']),'HFT'] = 1\n",
    "    MTF_target2['HFTVol'] = MTF_target2['HFT']*MTF_target2['vol1']\n",
    "    HFT_JoinRate = (MTF_target2.groupby(['date','Time_labels'])['HFTVol'].sum()/MTF_target2.groupby(['date','Time_labels'])['vol1'].sum())\n",
    "    HFT_JoinRate = HFT_JoinRate.reset_index().rename(columns={0:'HFT_JoinRate'})\n",
    "\n",
    "    #計算HFT流動性供需\n",
    "    MTF_targetSell = MTF_target2[MTF_target2['bs1']=='S']\n",
    "    MTF_targetSell['UpDown'] = MTF_targetSell['price1']<MTF_targetSell['price1'].shift()\n",
    "    MTF_targetSell['Fluidity'] = 0\n",
    "    MTF_targetSell['HFT_SellDemand'] = MTF_targetSell['UpDown']*MTF_targetSell['HFT']*MTF_targetSell['vol1']\n",
    "    HFT_SellDemand = (MTF_targetSell.groupby(['date','Time_labels'])['HFT_SellDemand'].sum()/MTF_targetSell['vol1'].sum()).reset_index()\n",
    "\n",
    "    MTF_targetBuy = MTF_target2[MTF_target2['bs1']=='B']\n",
    "    MTF_targetBuy['UpDown'] = MTF_targetBuy['price1']>MTF_targetBuy['price1'].shift()\n",
    "    MTF_targetBuy['Fluidity'] = 0\n",
    "    MTF_targetBuy['HFT_BuyDemand'] = MTF_targetBuy['UpDown']*MTF_targetBuy['HFT']*MTF_targetBuy['vol1']\n",
    "    HFT_BuyDemand = (MTF_targetBuy.groupby(['date','Time_labels'])['HFT_BuyDemand'].sum()/MTF_targetBuy['vol1'].sum()).reset_index()\n",
    "\n",
    "    MTF_targetSell['Supply'] = MTF_targetSell['price1']>MTF_targetSell['price1'].shift()\n",
    "    MTF_targetSell['HFT_SellSupply'] = MTF_targetSell['Supply']*MTF_targetSell['HFT']*MTF_targetSell['vol1']\n",
    "    HFT_SellSupply = (MTF_targetSell.groupby(['date','Time_labels'])['HFT_SellSupply'].sum()/MTF_targetSell['vol1'].sum()).reset_index()\n",
    "\n",
    "    MTF_targetBuy['Supply'] = MTF_targetBuy['price1']<MTF_targetBuy['price1'].shift()\n",
    "    MTF_targetBuy['HFT_BuySupply'] = MTF_targetBuy['Supply']*MTF_targetBuy['HFT']*MTF_targetBuy['vol1']\n",
    "    HFT_BuySupply = (MTF_targetBuy.groupby(['date','Time_labels'])['HFT_BuySupply'].sum()/MTF_targetBuy['vol1'].sum()).reset_index()\n",
    "\n",
    "    All1 = HFT_SellDemand.join(HFT_BuyDemand['HFT_BuyDemand'])\n",
    "    All1 = All1.join(HFT_JoinRate['HFT_JoinRate'])\n",
    "    All1['HFTDemand'] = All1['HFT_BuyDemand']+All1['HFT_SellDemand']\n",
    "\n",
    "    All2 = HFT_SellSupply.join(HFT_BuySupply['HFT_BuySupply'])\n",
    "    All2['HFTSupply'] = All2['HFT_BuySupply']+All2['HFT_SellSupply']\n",
    "    All = All1.join(All2['HFTSupply']).drop(columns=['HFT_BuyDemand','HFT_SellDemand'])\n",
    "    return HFT_JoinRate, All, MTF_target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "#切割MTF檔計算HFT參與率\n",
    "# 切割所屬樣本\n",
    "#先創一段時間樣本讓並記錄OSF委託所屬樣本區間\n",
    "def JoinRate1M():\n",
    "    Time_list = np.arange(845,1347,1)\n",
    "    Time_list = Time_list[Time_list%100 < 60]\n",
    "    Time_labels = np.delete(Time_list,0).astype(str)\n",
    "    MTF_target2 = MTF_target.copy()\n",
    "    MTF_target2['time'] = MTF_target2['daytime'].apply(lambda x: (x.hour)*100+x.minute)\n",
    "    MTF_target2['Time_labels'] = pd.cut(MTF_target2['time'], Time_list, right = False, labels = Time_labels)\n",
    "\n",
    "    #貼標記給所有HFT後計算參與率\n",
    "    MTF_target2['HFT'] = 0\n",
    "    for i in range(len(Record_HFT)):\n",
    "        MTF_target2.loc[(Record_HFT.loc[i,'date']==MTF_target2['date'])&(Record_HFT.loc[i,'id']==MTF_target2['id']),'HFT'] = 1\n",
    "    MTF_target2['HFTVol'] = MTF_target2['HFT']*MTF_target2['vol1']\n",
    "    HFT_JoinRate = (MTF_target2.groupby(['date','Time_labels'])['HFTVol'].sum()/MTF_target2.groupby(['date','Time_labels'])['vol1'].sum())\n",
    "    HFT_JoinRate = HFT_JoinRate.reset_index().rename(columns={0:'HFT_JoinRate'})\n",
    "\n",
    "    #計算HFT流動性供需\n",
    "    MTF_targetSell = MTF_target2[MTF_target2['bs1']=='S']\n",
    "    MTF_targetSell['UpDown'] = MTF_targetSell['price1']<MTF_targetSell['price1'].shift()\n",
    "    MTF_targetSell['Fluidity'] = 0\n",
    "    MTF_targetSell['HFT_SellDemand'] = MTF_targetSell['UpDown']*MTF_targetSell['HFT']*MTF_targetSell['vol1']\n",
    "    HFT_SellDemand = (MTF_targetSell.groupby(['date','Time_labels'])['HFT_SellDemand'].sum()/MTF_targetSell['vol1'].sum()).reset_index()\n",
    "\n",
    "    MTF_targetBuy = MTF_target2[MTF_target2['bs1']=='B']\n",
    "    MTF_targetBuy['UpDown'] = MTF_targetBuy['price1']>MTF_targetBuy['price1'].shift()\n",
    "    MTF_targetBuy['Fluidity'] = 0\n",
    "    MTF_targetBuy['HFT_BuyDemand'] = MTF_targetBuy['UpDown']*MTF_targetBuy['HFT']*MTF_targetBuy['vol1']\n",
    "    HFT_BuyDemand = (MTF_targetBuy.groupby(['date','Time_labels'])['HFT_BuyDemand'].sum()/MTF_targetBuy['vol1'].sum()).reset_index()\n",
    "\n",
    "    MTF_targetSell['Supply'] = MTF_targetSell['price1']>MTF_targetSell['price1'].shift()\n",
    "    MTF_targetSell['HFT_SellSupply'] = MTF_targetSell['Supply']*MTF_targetSell['HFT']*MTF_targetSell['vol1']\n",
    "    HFT_SellSupply = (MTF_targetSell.groupby(['date','Time_labels'])['HFT_SellSupply'].sum()/MTF_targetSell['vol1'].sum()).reset_index()\n",
    "\n",
    "    MTF_targetBuy['Supply'] = MTF_targetBuy['price1']<MTF_targetBuy['price1'].shift()\n",
    "    MTF_targetBuy['HFT_BuySupply'] = MTF_targetBuy['Supply']*MTF_targetBuy['HFT']*MTF_targetBuy['vol1']\n",
    "    HFT_BuySupply = (MTF_targetBuy.groupby(['date','Time_labels'])['HFT_BuySupply'].sum()/MTF_targetBuy['vol1'].sum()).reset_index()\n",
    "\n",
    "    All1 = HFT_SellDemand.join(HFT_BuyDemand['HFT_BuyDemand'])\n",
    "    All1 = All1.join(HFT_JoinRate['HFT_JoinRate'])\n",
    "    All1['HFTDemand'] = All1['HFT_BuyDemand']+All1['HFT_SellDemand']\n",
    "\n",
    "    All2 = HFT_SellSupply.join(HFT_BuySupply['HFT_BuySupply'])\n",
    "    All2['HFTSupply'] = All2['HFT_BuySupply']+All2['HFT_SellSupply']\n",
    "    All = All1.join(All2['HFTSupply']).drop(columns=['HFT_BuyDemand','HFT_SellDemand'])\n",
    "    return HFT_JoinRate, All, MTF_target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#NearDepth\n",
    "def NearDepth5M():\n",
    "    #NearDepth\n",
    "    Time_list = [845, 850, 855, 900, 905 ,910, 915, 920, 925, 930, 935, 940, 945, 950, 955, 1000, 1005, 1010, 1015, 1020, 1025, 1030, 1035,\n",
    "                1040, 1045, 1050, 1055, 1100, 1105, 1110, 1115, 1120, 1125, 1130, 1135, 1140, 1145, 1150, 1155, 1200, 1205, 1210,1215,\n",
    "                1220, 1225, 1230, 1235, 1240, 1245, 1250, 1255, 1300, 1305, 1310, 1315, 1320, 1325, 1330, 1335, 1340, 1346]\n",
    "    Time_labels =['850', '855', \"900\", \"905\" ,\"910\", \"915\", \"920\", \"925\", \"930\", \"935\", \"940\", \"945\", \"950\", \"955\", \"1000\", \"1005\", \"1010\",\n",
    "                  \"1015\", \"1020\", \"1025\", \"1030\", \"1035\", \"1040\", \"1045\", \"1050\", \"1055\", \"1100\", \"1105\", \"1110\", '1115', '1120', '1125', \n",
    "                  '1130', '1135', '1140', '1145', '1150', '1155', '1200', '1205', '1210', '1215', '1220', '1225', '1230', '1235', '1240',\n",
    "                  '1245', '1250', '1255', '1300', '1305', '1310', '1315', '1320', '1325', '1330', '1335', '1340', '1346']\n",
    "    DSP_target2 = DSP_target\n",
    "\n",
    "    DSP_target2['NearDepth'] = (DSP_target2['bidv1']+DSP_target2['bidv2']+DSP_target2['bidv3']+DSP_target2['bidv4']+DSP_target2['bidv5']\n",
    "                                +DSP_target2['askv1']+DSP_target2['askv2']+DSP_target2['askv3']+DSP_target2['askv4']+DSP_target2['askv5'])\n",
    "    DSP_target2['time']= DSP_target2['daytime'].apply(lambda x: (x.hour)*100+x.minute)\n",
    "    DSP_target2['Time_labels'] = pd.cut(DSP_target2['time'], Time_list, right = False, labels = Time_labels)\n",
    "\n",
    "    DSP_target3 = DSP_target2[['date','daytime','NearDepth','Time_labels']]\n",
    "    DSP_target3 = DSP_target3.groupby(['date','daytime','Time_labels'])['NearDepth'].mean().reset_index() #因為每個row都是一秒所以直接平均就可以了\n",
    "    NearDepth = DSP_target3[['date','Time_labels','NearDepth']].groupby(['date','Time_labels'])['NearDepth'].sum().reset_index()\n",
    "\n",
    "    #計算市場品質HighLow\n",
    "    DSP_target4 = DSP_target2[['date','daytime','bid1','bidv1','ask1','askv1','time','Time_labels']]\n",
    "\n",
    "    DSP_target4['MidPoint'] = (DSP_target4['bid1']+DSP_target4['ask1'])/2\n",
    "    MidPointRange = (DSP_target4.groupby(['date','Time_labels'])['MidPoint'].max()-DSP_target4.groupby(['date','Time_labels'])['MidPoint'].min()\n",
    "                    ).reset_index().rename(columns={'MidPoint':'MidPointRange'})\n",
    "    MidPointMean = DSP_target4.groupby(['date','daytime','Time_labels'])['MidPoint'].mean().reset_index().drop(['daytime'],axis= 1)          #每個row都是一秒所以直接平均\n",
    "    MidPointHighLow = pd.merge(MidPointRange,MidPointMean,on=['date','Time_labels'])\n",
    "    MidPointHighLow['MidPointHighLow'] = MidPointHighLow['MidPointRange']/MidPointHighLow['MidPoint']\n",
    "    MidPointHighLow = MidPointHighLow.groupby(['date','Time_labels'])['MidPointHighLow'].max().reset_index()\n",
    "\n",
    "    DSP_target4['VolMidPoint'] = (DSP_target4['bid1']*DSP_target4['bidv1']+DSP_target4['ask1']*DSP_target4['askv1'])/(DSP_target4['bidv1']+DSP_target4['askv1'])\n",
    "    VolMidPointRange = (DSP_target4.groupby(['date','Time_labels'])['VolMidPoint'].max()-DSP_target4.groupby(['date','Time_labels'])['VolMidPoint'].min()\n",
    "                             ).reset_index().rename(columns={'VolMidPoint':'VolMidPointRange'})\n",
    "    VolweightMidPointMean = DSP_target4.groupby(['date','daytime','Time_labels'])['VolMidPoint'].mean().reset_index().drop(['daytime'],axis= 1)\n",
    "    VolMidPointHighLow = pd.merge(VolMidPointRange,VolweightMidPointMean,on=['date','Time_labels'])\n",
    "    VolMidPointHighLow['VolMidPointHighLow'] = VolMidPointHighLow['VolMidPointRange']/VolMidPointHighLow['VolMidPoint']\n",
    "    VolMidPointHighLow = VolMidPointHighLow.groupby(['date','Time_labels'])['VolMidPointHighLow'].max().reset_index()\n",
    "\n",
    "    #計算市場品質Volatility\n",
    "    RollingPeriod = 60\n",
    "    Volatility = MTF_target2[['date','daytime','price1','vol1' ,'Time_labels']].sort_values(by=['daytime'])\n",
    "    Volatility['VolatilityN'] = Volatility['price1'].rolling(RollingPeriod).std()\n",
    "    VolatilityN = Volatility.groupby(['date','Time_labels']).mean().reset_index().drop(['price1','vol1'],axis= 1)\n",
    "\n",
    "    #時間加權Volatility微調時間經過\n",
    "    modify = Volatility.groupby(['date'])['price1'].last().reset_index()\n",
    "    modify['date']+=datetime.timedelta(days=1)\n",
    "    modify['time'] = datetime.time(8,45)\n",
    "    modify['vol1'] = 0\n",
    "    Volatility1 = Volatility.groupby(['date','daytime','price1',\"Time_labels\"])['vol1'].sum().reset_index()\n",
    "    Volatility1['time'] = Volatility1['daytime'].apply(lambda x:x.time())\n",
    "    Volatility1 = Volatility1.drop(['daytime'],axis= 1)\n",
    "    Volatility1 = pd.concat([Volatility1,modify],axis=0).sort_values(by=['date','time'])\n",
    "    Volatility1[\"time\"] = Volatility1[\"time\"].apply(lambda x:datetime.datetime.combine(datetime.datetime.min,x)-datetime.datetime.min)\n",
    "    Volatility1['timelag'] = Volatility1.groupby(['date'])['time'].shift() \n",
    "    Volatility1['timedelta'] = Volatility1['time']-Volatility1['timelag']\n",
    "    Volatility1['VolatilityN']= Volatility1['price1'].rolling(RollingPeriod,min_periods=1).std()\n",
    "    Volatility1.loc[:,'VolatilityT'] = Volatility1.loc[:,'VolatilityN']*Volatility1.loc[:,'timedelta'].apply(lambda x:x.microseconds)/300000\n",
    "    VolatilityT = (Volatility1.groupby(['date','Time_labels'])['VolatilityT'].mean()).reset_index()\n",
    "    VolatilityT['Time_labels'] = VolatilityT['Time_labels'].apply(lambda x:int(x))\n",
    "    VolatilityT = VolatilityT.sort_values(by=['date','Time_labels']).reset_index(drop=True)\n",
    "    VolatilityT['Time_labels'] = VolatilityT['Time_labels'].apply(lambda x:str(x))\n",
    "    return VolatilityT, VolatilityN, NearDepth, MidPointHighLow, VolMidPointHighLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NearDepth\n",
    "def NearDepth1M():\n",
    "    Time_list = np.arange(845,1347,1)\n",
    "    Time_list = Time_list[Time_list%100 < 60]\n",
    "    Time_labels = np.delete(Time_list,0).astype(str)\n",
    "    DSP_target2 = DSP_target\n",
    "\n",
    "    DSP_target2['NearDepth'] = (DSP_target2['bidv1']+DSP_target2['bidv2']+DSP_target2['bidv3']+DSP_target2['bidv4']+DSP_target2['bidv5']\n",
    "                                +DSP_target2['askv1']+DSP_target2['askv2']+DSP_target2['askv3']+DSP_target2['askv4']+DSP_target2['askv5'])\n",
    "    DSP_target2['time']= DSP_target2['daytime'].apply(lambda x: (x.hour)*100+x.minute)\n",
    "    DSP_target2['Time_labels'] = pd.cut(DSP_target2['time'], Time_list, right = False, labels = Time_labels)\n",
    "\n",
    "    DSP_target3 = DSP_target2[['date','daytime','NearDepth','Time_labels']]\n",
    "    DSP_target3 = DSP_target3.groupby(['date','daytime','Time_labels'])['NearDepth'].mean().reset_index() #因為每個row都是一秒所以直接平均就可以了\n",
    "    NearDepth = DSP_target3[['date','Time_labels','NearDepth']].groupby(['date','Time_labels'])['NearDepth'].sum().reset_index()\n",
    "\n",
    "    #計算市場品質HighLow\n",
    "    DSP_target4 = DSP_target2[['date','daytime','bid1','bidv1','ask1','askv1','time','Time_labels']]\n",
    "\n",
    "    DSP_target4['MidPoint'] = (DSP_target4['bid1']+DSP_target4['ask1'])/2\n",
    "    MidPointRange = (DSP_target4.groupby(['date','Time_labels'])['MidPoint'].max()-DSP_target4.groupby(['date','Time_labels'])['MidPoint'].min()\n",
    "                    ).reset_index().rename(columns={'MidPoint':'MidPointRange'})\n",
    "    MidPointMean = DSP_target4.groupby(['date','daytime','Time_labels'])['MidPoint'].mean().reset_index().drop(['daytime'],axis= 1)          #每個row都是一秒所以直接平均\n",
    "    MidPointHighLow = pd.merge(MidPointRange,MidPointMean,on=['date','Time_labels'])\n",
    "    MidPointHighLow['MidPointHighLow'] = MidPointHighLow['MidPointRange']/MidPointHighLow['MidPoint']\n",
    "    MidPointHighLow = MidPointHighLow.groupby(['date','Time_labels'])['MidPointHighLow'].max().reset_index()\n",
    "\n",
    "    DSP_target4['VolMidPoint'] = (DSP_target4['bid1']*DSP_target4['bidv1']+DSP_target4['ask1']*DSP_target4['askv1'])/(DSP_target4['bidv1']+DSP_target4['askv1'])\n",
    "    VolMidPointRange = (DSP_target4.groupby(['date','Time_labels'])['VolMidPoint'].max()-DSP_target4.groupby(['date','Time_labels'])['VolMidPoint'].min()\n",
    "                             ).reset_index().rename(columns={'VolMidPoint':'VolMidPointRange'})\n",
    "    VolweightMidPointMean = DSP_target4.groupby(['date','daytime','Time_labels'])['VolMidPoint'].mean().reset_index().drop(['daytime'],axis= 1)\n",
    "    VolMidPointHighLow = pd.merge(VolMidPointRange,VolweightMidPointMean,on=['date','Time_labels'])\n",
    "    VolMidPointHighLow['VolMidPointHighLow'] = VolMidPointHighLow['VolMidPointRange']/VolMidPointHighLow['VolMidPoint']\n",
    "    VolMidPointHighLow = VolMidPointHighLow.groupby(['date','Time_labels'])['VolMidPointHighLow'].max().reset_index()\n",
    "\n",
    "    #計算市場品質Volatility\n",
    "    RollingPeriod = 60\n",
    "    Volatility = MTF_target2[['date','daytime','price1','vol1' ,'Time_labels']].sort_values(by=['daytime'])\n",
    "    Volatility['VolatilityN'] = Volatility['price1'].rolling(RollingPeriod).std()\n",
    "    VolatilityN = Volatility.groupby(['date','Time_labels']).mean().reset_index().drop(['price1','vol1'],axis= 1)\n",
    "\n",
    "    #時間加權Volatility微調時間經過\n",
    "    modify = Volatility.groupby(['date'])['price1'].last().reset_index()\n",
    "    modify['date']+=datetime.timedelta(days=1)\n",
    "    modify['time'] = datetime.time(8,45)\n",
    "    modify['vol1'] = 0\n",
    "    Volatility1 = Volatility.groupby(['date','daytime','price1',\"Time_labels\"])['vol1'].sum().reset_index()\n",
    "    Volatility1['time'] = Volatility1['daytime'].apply(lambda x:x.time())\n",
    "    Volatility1 = Volatility1.drop(['daytime'],axis= 1)\n",
    "    Volatility1 = pd.concat([Volatility1,modify],axis=0).sort_values(by=['date','time'])\n",
    "    Volatility1[\"time\"] = Volatility1[\"time\"].apply(lambda x:datetime.datetime.combine(datetime.datetime.min,x)-datetime.datetime.min)\n",
    "    Volatility1['timelag'] = Volatility1.groupby(['date'])['time'].shift() \n",
    "    Volatility1['timedelta'] = Volatility1['time']-Volatility1['timelag']\n",
    "    Volatility1['VolatilityN']= Volatility1['price1'].rolling(RollingPeriod,min_periods=1).std()\n",
    "    Volatility1.loc[:,'VolatilityT'] = Volatility1.loc[:,'VolatilityN']*Volatility1.loc[:,'timedelta'].apply(lambda x:x.microseconds)/300000\n",
    "    VolatilityT = (Volatility1.groupby(['date','Time_labels'])['VolatilityT'].mean()).reset_index()\n",
    "    VolatilityT['Time_labels'] = VolatilityT['Time_labels'].apply(lambda x:int(x))\n",
    "    VolatilityT = VolatilityT.sort_values(by=['date','Time_labels']).reset_index(drop=True)\n",
    "    VolatilityT['Time_labels'] = VolatilityT['Time_labels'].apply(lambda x:str(x))\n",
    "    return VolatilityT, VolatilityN, NearDepth, MidPointHighLow, VolMidPointHighLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Total_Variable(name):\n",
    "    #控制變數\n",
    "    Control_Return = MTF_target2.groupby(['date','Time_labels'])['price1'].last().reset_index()\n",
    "    Control_Return['return'] = Control_Return['price1'].pct_change()\n",
    "    Control_Return = Control_Return.drop(['price1'],axis= 1)\n",
    "\n",
    "    Control_Volume = MTF_target2.groupby(['date','Time_labels'])['vol1'].sum().reset_index().rename(columns={'vol1':'volume'})\n",
    "\n",
    "    #匯集全部變數\n",
    "    Ttotal_Variable = All.join(NearDepth['NearDepth'])\n",
    "    Ttotal_Variable = Ttotal_Variable.join(MidPointHighLow['MidPointHighLow'])\n",
    "    Ttotal_Variable = Ttotal_Variable.join(VolMidPointHighLow['VolMidPointHighLow'])\n",
    "    Ttotal_Variable = Ttotal_Variable.join(VolatilityN['VolatilityN'])\n",
    "    Ttotal_Variable = Ttotal_Variable.join(VolatilityT['VolatilityT'])\n",
    "    Ttotal_Variable = Ttotal_Variable.join(Control_Return['return'])\n",
    "    Ttotal_Variable = Ttotal_Variable.join(Control_Volume['volume'])\n",
    "    Ttotal_Variable[\"return\"] = Ttotal_Variable[\"return\"].fillna(0)\n",
    "    Ttotal_Variable.to_csv('Ttotal_Variable_'+ date +name+'.csv')\n",
    "    return Ttotal_Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading(name):\n",
    "    while True:\n",
    "        for j in name:\n",
    "            if j != \"POI\":\n",
    "                Folder_Path = initial_place + \"\\\\\" + j\n",
    "                os.chdir(Folder_Path)\n",
    "                file_list = np.array(os.listdir())\n",
    "                file_list = file_list[start_index:end_index]\n",
    "                yield pd.read_csv(Folder_Path + '\\\\'+ file_list[i],sep=',',encoding='big5')\n",
    "            elif j == \"POI\":\n",
    "                yield j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 併發holding period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holding_period():\n",
    "    POI_target1 = POI_target[['date','id','daytime','contract','ybuyre','ysellre']]\n",
    "    POI_target1['daytime'] +=datetime.timedelta(hours=8, minutes=45)\n",
    "    POI_target1['oc'] = 0\n",
    "    POI_target1['vol'] = POI_target1['ybuyre']+POI_target1['ysellre']     #不論昨日多空各留多少只要沒看到平倉就要計算持倉時間\n",
    "    POI_target1 = POI_target1.drop(['ybuyre','ysellre'],axis= 1)\n",
    "\n",
    "    #增加最後一行方便計算持倉到收盤的口數\n",
    "    POI_target2 = POI_target[['date','id','daytime','contract','tbuyre','tsellre']]\n",
    "    POI_target2['daytime'] += datetime.timedelta(hours=13,minutes=44, seconds=59, milliseconds =999)\n",
    "    POI_target2['oc'] = 3\n",
    "    POI_target2['vol'] = POI_target2['tbuyre']+POI_target2['tsellre']     #不論今日多空各留多少只要沒看到平倉就要計算持倉時間\n",
    "    POI_target2 = POI_target2.drop(['tbuyre','tsellre'],axis= 1)\n",
    "\n",
    "    #處理成交檔再concat計算持倉時間\n",
    "    MTF_target1 = MTF_target[['date','id','daytime','contract1','oc','vol1']].rename(columns={'vol1':'vol','contract1':'contract'})\n",
    "    Calculate_df_cols = ['date','daytime','contract','id','oc','vol','timedelta','OpenInterest','time_lag','InterestChange','CumulativeTime','TotalOpen']\n",
    "    Calculate_temp = pd.DataFrame(columns=Calculate_df_cols)\n",
    "    Calculate_df = pd.concat([Calculate_temp,POI_target1,POI_target2,MTF_target1],axis=0).sort_values(['daytime']).reset_index(drop=True)\n",
    "    Calculate_df = Calculate_df.ix[:,Calculate_df_cols]\n",
    "    Calculate_df2 = pd.DataFrame(columns = Calculate_df_cols)\n",
    "\n",
    "    def check_yield():\n",
    "        while True: \n",
    "            b = yield\n",
    "            if b<0:\n",
    "                Calculate_df1[a,5] += abs(b)\n",
    "                Calculate_df1[:,9] = Calculate_df1[:,5]*(((Calculate_df1[:,4]==0)|(Calculate_df1[:,4]==2))*2-1)\n",
    "    Datelist = pd.unique(POI_target['date'])\n",
    "    for i in Datelist:\n",
    "        if i != date_now:\n",
    "            continue\n",
    "        Calculate_df1 = Calculate_df[Calculate_df['date']==i]\n",
    "        Calculate_df1.loc[:,'InterestChange'] = (Calculate_df1['vol']*Calculate_df1['oc'].apply(lambda x:((x==0)or(x==2))*2-1)).astype(float)\n",
    "        Calculate_df1.loc[:,'time_lag'] = Calculate_df1.groupby(['id'])['daytime'].shift(1)\n",
    "        Calculate_df1.loc[:,'OpenInterest'] = Calculate_df1.groupby(['date','id'])['InterestChange'].cumsum()-Calculate_df1['InterestChange']\n",
    "        Calculate_df1 = Calculate_df1.values\n",
    "        Calculate_df1[:,6] = (Calculate_df1[:,1] - Calculate_df1[:,8])\n",
    "        Idlist = pd.unique(Calculate_df1[:,3])\n",
    "        c = check_yield()\n",
    "        next(c)\n",
    "        for j in Idlist:\n",
    "            a = np.where(Calculate_df1[:,3]==j)[0][0]\n",
    "            b = np.sort(Calculate_df1[Calculate_df1[:,3]==j][:,7])[0]\n",
    "            c.send(b)\n",
    "        Calculate_df1 = pd.DataFrame(Calculate_df1,columns = Calculate_df_cols)\n",
    "        Calculate_df1.loc[:,'InterestChange'] = Calculate_df1['InterestChange'].astype(float)\n",
    "        Calculate_df1.loc[:,'OpenInterest'] = Calculate_df1.groupby(['id'])['InterestChange'].cumsum()-Calculate_df1['InterestChange']\n",
    "        Calculate_df1.loc[:,'CumulativeTime'] = Calculate_df1['timedelta']*Calculate_df1['OpenInterest']\n",
    "        Calculate_df1.loc[:,'TotalOpen'] = Calculate_df1['vol']*Calculate_df1['oc'].apply(lambda x:((x==0)or(x==2)))\n",
    "        Calculate_df1 = Calculate_df1.fillna(0)\n",
    "        Calculate_df2 = Calculate_df2.append(Calculate_df1)\n",
    "\n",
    "    HoldingPeriod = pd.merge(Calculate_df2.groupby(['date','id'])['CumulativeTime'].sum().reset_index(),Calculate_df2.groupby(['date','id'])['TotalOpen'].sum().reset_index(), on=['date','id'], how='outer')\n",
    "    HoldingPeriod['TotalOpen'] = HoldingPeriod['TotalOpen'].astype(float)\n",
    "    HoldingPeriod['HoldingPeriod'] = (HoldingPeriod['CumulativeTime']/HoldingPeriod['TotalOpen'])\n",
    "    HoldingPeriod = HoldingPeriod.dropna()\n",
    "    return HoldingPeriod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請Total檔案放置與4個檔案資料夾(DSP,MTF,OSF,POI])同一階層接收資料\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: FutureWarning: Passing integers to fillna is deprecated, will raise a TypeError in a future version.  To retain the old behavior, pass pd.Timedelta(seconds=n) instead.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: Passing integers to fillna is deprecated, will raise a TypeError in a future version.  To retain the old behavior, pass pd.Timedelta(seconds=n) instead.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:56: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.62701511383057\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"請Total檔案放置與4個檔案資料夾(DSP,MTF,OSF,POI])同一階層接收資料\")\n",
    "       #要拼接的文件夹及其完整路径，注意不要包含中文\n",
    "os.chdir(initial_place + \"\\\\\" + \"poi\")\n",
    "poi_list = np.array(os.listdir())\n",
    "poi_list = poi_list[np.where(poi_list == poi_file_start)[0][0]:np.where(poi_list == poi_file_end)[0][0]+1]\n",
    "# poi_start = np.where(poi_list == poi_file_start)[0][0]\n",
    "# poi_end =  np.where(poi_list == poi_file_end)[0][0]+1\n",
    "\n",
    "os.chdir(initial_place + \"\\\\\" + \"OSF\")\n",
    "start_index = np.where(file_list == start_file)[0][0]\n",
    "end_index = np.where(file_list == end_file)[0][0]+1\n",
    "file_list = file_list[start_index:end_index]\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    date_now = \"\"\n",
    "    os.chdir(initial_place + \"\\\\\" + \"OSF\")\n",
    "#     start_index = np.where(file_list == start_file)[0][0]\n",
    "#     end_index = np.where(file_list == end_file)[0][0]+1\n",
    "    file_list = os.listdir()\n",
    "    file_list = file_list[start_index:end_index]\n",
    "    try:\n",
    "        transfer_list = [file_list[i][-12:-4],file_list[i+1][-12:-4],file_list[i+2][-12:-4]]\n",
    "    except:\n",
    "        try:\n",
    "            transfer_list = [file_list[i][-12:-4],file_list[i+1][-12:-4]]\n",
    "        except:\n",
    "            transfer_list = [file_list[i][-12:-4]]\n",
    "    year_month_now = file_list[i][-12:-6]\n",
    "    date = file_list[i][-12:-4]\n",
    "    date_now = datetime.datetime.strptime(date,\"%Y%m%d\").date()\n",
    "    #將所有文件名存入一个列表\n",
    "    read = reading(name)\n",
    "\n",
    "    OSF_RawData = next(read)\n",
    "    MTF_RawData = next(read)\n",
    "    DSP_RawData = next(read)\n",
    "\n",
    "    POI = next(read)\n",
    "    Folder_Path = initial_place + \"\\\\\" + POI\n",
    "    if poi_true == True:\n",
    "        POI_RawData = pd.read_csv(Folder_Path + '\\\\'+ poi_list[poi_count],sep=',',encoding='big5')\n",
    "\n",
    "        poi_true =False\n",
    "    if year_month_now not in poi_list[poi_count]:\n",
    "        poi_count += 1\n",
    "        POI_RawData = pd.read_csv(Folder_Path + '\\\\'+ poi_list[poi_count],sep=',',encoding='big5')\n",
    "    year_month_now = file_list[i][-12:-6]\n",
    "    date = file_list[i][-12:-4]\n",
    "    date_now = datetime.datetime.strptime(date,\"%Y%m%d\").date()\n",
    "    OSF_RawData['daytime'] = pd.to_datetime(OSF_RawData['daytime'] , format='%Y-%m-%d %H:%M:%S')\n",
    "    MTF_RawData['daytime'] = pd.to_datetime(MTF_RawData['daytime'] , format='%Y-%m-%d %H:%M:%S')\n",
    "    DSP_RawData['daytime'] = pd.to_datetime(DSP_RawData['daytime'] , format='%Y-%m-%d %H:%M:%S')\n",
    "    POI_RawData['daytime'] = pd.to_datetime(POI_RawData['daytime'].astype(str), format='%Y-%m-%d')\n",
    "    OSF_TXF, MTF_TXF, DSP_TXF, POI_TXF = Data_Pick()\n",
    "    OSF_target, POI_target, MTF_target, DSP_target = Transfer()\n",
    "    HoldingPeriod = holding_period()\n",
    "    Record_HFT,RankData = Vol_Calculate()\n",
    "    HFT_JoinRate, All, MTF_target2 = JoinRate5M()\n",
    "    VolatilityT, VolatilityN, NearDepth, MidPointHighLow, VolMidPointHighLow = NearDepth5M()\n",
    "    SaveFile_Path =  initial_place\n",
    "    os.chdir(SaveFile_Path  + \"Features_record\")\n",
    "    Features_record = Features_Record(\"\")\n",
    "    os.chdir(SaveFile_Path  + \"Ttotal_Variable\")\n",
    "    Ttotal_Variable = Total_Variable(\"_5m\")\n",
    "    gc.collect()\n",
    "    HFT_JoinRate, All, MTF_target2 = JoinRate1M()\n",
    "    VolatilityT, VolatilityN, NearDepth, MidPointHighLow, VolMidPointHighLow = NearDepth1M()\n",
    "    # 位置沒改變不須切換\n",
    "    Ttotal_Variable = Total_Variable(\"_1m\")\n",
    "\n",
    "    \n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合併檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_merge(j):\n",
    "    while True:\n",
    "        yield pd.read_csv(file_list[j+1],encoding='big5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 併feature只要改位置跟最後儲存名稱即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(initial_place + \"\\\\\" + \"Ttotal_Variable\")\n",
    "file_list = np.array(os.listdir())\n",
    "Total_merge = pd.read_csv(file_list[0])\n",
    "for j in range(len(file_list)-1):\n",
    "    RM = read_merge(j)\n",
    "    temp = next(RM)\n",
    "    Total_merge = Total_merge.append(temp)\n",
    "    RM.send(j)\n",
    "Total_merge = Total_merge.reset_index(drop = True)\n",
    "Total_merge.to_csv(\"Total_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(initial_place + \"\\\\\" + \"Features_record\")\n",
    "file_list = np.array(os.listdir())\n",
    "# 需要使用變數記得改這邊的變數名稱\n",
    "Total_merge = pd.read_csv(file_list[0])\n",
    "for j in range(len(file_list)-1):\n",
    "    RM = read_merge(j)\n",
    "    Total_merge = (Total_merge + next(RM))/len(file_list)\n",
    "    RM.send(j)\n",
    "Total_merge = Total_merge.reset_index(drop = True)\n",
    "Total_merge.to_csv(\"Features_record.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
